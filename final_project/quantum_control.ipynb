{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "Using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import qutip as qt\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.tensordict import TensorDict, TensorDictBase\n",
    "from torch import nn\n",
    "\n",
    "import qcontrol\n",
    "device = qcontrol.device\n",
    "print(\"Using device\", device)\n",
    "\n",
    "# Set seed for repeatability\n",
    "env = qcontrol.QuantumEnv()\n",
    "torch.manual_seed(0)\n",
    "env.set_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.005\n",
    "gate_time = 10\n",
    "control_thresh = 1\n",
    "control_penalty = 2\n",
    "close_thresh = 0.99999\n",
    "\n",
    "np_ctype = np.complex64\n",
    "psi_0 = np.array([[1,  0],\n",
    "                    [0,  1]]).astype(np_ctype)\n",
    "psi_f = np.array([[0,  1],\n",
    "                    [1,  0]]).astype(np_ctype)\n",
    "H0 = np.array([[1,  0],\n",
    "                [0, -1]]).astype(np_ctype)\n",
    "H1 = np.array([[0,  1],\n",
    "                [1,  0]]).astype(np_ctype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class agentNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(agentNet, self).__init__()\n",
    "        ctype = torch.float32\n",
    "        p_drop = 0.01\n",
    "        bias = True\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(2*psi_0.size, 64, dtype=ctype, bias=bias),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(64, 64, dtype=ctype, bias=bias),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Dropout(p=p_drop),\n",
    "                    nn.Linear(64, 16, dtype=ctype, bias=bias),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Dropout(p=p_drop),\n",
    "                    nn.Linear(16, 1, dtype=ctype, bias=bias),\n",
    "                    )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        flattened = torch.hstack((x1.flatten(1, -1), x2.flatten(1, -1)))\n",
    "        return self.net(flattened)\n",
    "\n",
    "agent = agentNet().to(device)\n",
    "policy = TensorDictModule(agent,\n",
    "                          in_keys=[\"psi_real\", \"psi_imag\"],\n",
    "                           out_keys=[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reward: -2.8557, last reward: -1.8894, gradient norm:  4.995:   4%|â–         | 4/100 [00:11<04:38,  2.90s/it]"
     ]
    }
   ],
   "source": [
    " optim = torch.optim.Adam(policy.parameters(), lr=2e-3)\n",
    "\n",
    "# Train Parameters\n",
    "batch_size = 100\n",
    "rollout_len = 1500\n",
    "total_trials = 10000\n",
    "pbar = tqdm.tqdm(range(total_trials // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_trials)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "\n",
    "params = env.gen_params(batch_size=[batch_size], dt=dt,\n",
    "                        gate_time=gate_time, control_thresh=control_thresh,\n",
    "                        control_penalty=control_penalty, psi_0=psi_0,\n",
    "                        psi_f=psi_f, H0=H0, H1=H1, close_thresh=close_thresh)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(params)\n",
    "    rollout = env.rollout(rollout_len, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(logs[\"return\"])\n",
    "plt.title(\"returns\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(logs[\"last_reward\"])\n",
    "plt.title(\"last reward\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bewdesktop/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bewdesktop/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     init_td \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(params)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bewdesktop/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     rollout \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrollout(rollout_len, policy, tensordict\u001b[39m=\u001b[39;49minit_td, auto_reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bewdesktop/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     traj_return \u001b[39m=\u001b[39m rollout[\u001b[39m\"\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bewdesktop/home/eweissler/src/neural_networks_hw/final_project/quantum_control.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     (\u001b[39m-\u001b[39mtraj_return)\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torchrl/envs/common.py:1814\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, out)\u001b[0m\n\u001b[1;32m   1804\u001b[0m kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtensordict\u001b[39m\u001b[39m\"\u001b[39m: tensordict,\n\u001b[1;32m   1806\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mauto_cast_to_device\u001b[39m\u001b[39m\"\u001b[39m: auto_cast_to_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m\"\u001b[39m: callback,\n\u001b[1;32m   1812\u001b[0m }\n\u001b[1;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m break_when_any_done:\n\u001b[0;32m-> 1814\u001b[0m     tensordicts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rollout_stop_early(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     tensordicts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rollout_nonstop(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torchrl/envs/common.py:1839\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[39mif\u001b[39;00m auto_cast_to_device:\n\u001b[1;32m   1838\u001b[0m     tensordict \u001b[39m=\u001b[39m tensordict\u001b[39m.\u001b[39mto(policy_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 1839\u001b[0m tensordict \u001b[39m=\u001b[39m policy(tensordict)\n\u001b[1;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m auto_cast_to_device:\n\u001b[1;32m   1841\u001b[0m     tensordict \u001b[39m=\u001b[39m tensordict\u001b[39m.\u001b[39mto(env_device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/nn/functional_modules.py:588\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    590\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/nn/common.py:282\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/nn/utils.py:254\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     skip_existing()\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    251\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/nn/common.py:1164\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(tensordict\u001b[39m.\u001b[39mget(in_key, \u001b[39mNone\u001b[39;00m) \u001b[39mfor\u001b[39;00m in_key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys)\n\u001b[1;32m   1163\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/tensordict/nn/common.py:1121\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[0;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_module\u001b[39m(\n\u001b[1;32m   1119\u001b[0m     \u001b[39mself\u001b[39m, tensors: Sequence[Tensor], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor \u001b[39m|\u001b[39m Sequence[Tensor]:\n\u001b[0;32m-> 1121\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dt = 0.005\n",
    "timesteps = 1500\n",
    "tlist = np.arange(timesteps)*dt\n",
    "\n",
    "def get_control_pulse(net, timesteps=timesteps, dt=dt, ctype=torch.complex64):\n",
    "    \n",
    "    net.eval()\n",
    "    psi = torch.tensor(psi_0, dtype=ctype, device=device)\n",
    "    H0_tensor = torch.tensor(H0, dtype=ctype, device=device)\n",
    "    H1_tensor = torch.tensor(H1, dtype=ctype, device=device)\n",
    "    cntrl_seq = torch.zeros(timesteps, dtype=ctype)\n",
    "    psi_list = []\n",
    "    for i in range(timesteps):\n",
    "        cntrl = net(torch.real(psi).reshape((1,4,1)), torch.imag(psi).reshape((1,4,1)))\n",
    "        cntrl_seq[i] = cntrl\n",
    "        U = torch.linalg.matrix_exp(-1.0j*dt*(H0_tensor+cntrl*H1_tensor))\n",
    "        psi = U@psi\n",
    "        psi_list.append(psi.cpu().detach().numpy())\n",
    "\n",
    "    return cntrl_seq.cpu().detach().numpy(), psi_list\n",
    "\n",
    "pulse, psi_list = get_control_pulse(net)\n",
    "\n",
    "def Ham(t, tlist):\n",
    "    tdiff = np.abs(tlist - t)\n",
    "    return qt.Qobj(H0 + pulse[np.argmin(tdiff)]*H1)\n",
    "\n",
    "states_qt = qt.sesolve(lambda t, args: Ham(t, tlist), qt.Qobj(psi_0), tlist=tlist)\n",
    "prob0_qt = [np.abs(s[0][0][0])**2 for s in states_qt.states]\n",
    "\n",
    "prob0_start0 = [np.abs(s[0,0])**2 for s in psi_list]\n",
    "prob0_start1 = [np.abs(s[0,1])**2 for s in psi_list]\n",
    "\n",
    "f, ax = plt.subplots(ncols=2)\n",
    "ax[0].set_title(\"pulse\")\n",
    "ax[1].set_title(\"population in [1,0] state\")\n",
    "ax[0].plot(tlist, pulse)\n",
    "# ax[1].plot(tlist, prob0_qt, label=\"qutip\")\n",
    "ax[1].plot(tlist, prob0_start0, label=\"starts in [1,0]\")\n",
    "ax[1].plot(tlist, prob0_start1, label=\"starts in [0,1]\")\n",
    "ax[1].legend()\n",
    "plt.savefig(\"pulse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
